name: GitHub Crawler Pipeline

on:
  workflow_dispatch:   # allows manual trigger
  schedule:
    - cron: "0 * * * *"   # runs every hour (UTC)

concurrency:
  group: github-crawler
  cancel-in-progress: false

jobs:
  crawl-stars:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Checkout repo code
      - name: Checkout code
        uses: actions/checkout@v3

      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Force IPv4 for Supabase host
        run: |
          IPV4=$(getent ahostsv4 ${{ secrets.PGHOST }} | head -n 1 | awk '{print $1}')
          echo "$IPV4  ${{ secrets.PGHOST }}" | sudo tee -a /etc/hosts
          
      # Step 4: Run crawler
      - name: Run GitHub crawler
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PGHOST: ${{ secrets.PGHOST }}
          PGPORT: ${{ secrets.PGPORT }}
          PGUSER: ${{ secrets.PGUSER }}
          PGPASSWORD: ${{ secrets.PGPASSWORD }}
          PGDATABASE: ${{ secrets.PGDATABASE }}
        run: |
          python main.py

      # Step 5: Export results to CSV
      - name: Export DB to CSV
        run: |
          sudo apt-get update && sudo apt-get install -y postgresql-client
          psql "postgresql://${{ secrets.PGUSER }}:${{ secrets.PGPASSWORD }}@${{ secrets.PGHOST }}:${{ secrets.PGPORT }}/${{ secrets.PGDATABASE }}" \
          -c "\COPY repositories TO 'repos.csv' CSV HEADER"
      # Step 6: Upload CSV as artifact
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: repo-data
          path: repos.csv
